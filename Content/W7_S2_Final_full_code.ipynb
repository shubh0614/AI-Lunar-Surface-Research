{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# <center> Building and Training a UNet Model (W7 S2)"
      ],
      "metadata": {
        "id": "xksEYwr8QXgN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### About the Dataset\n",
        "\n",
        "This dataset contains 9766 realistic renders of lunar landscapes and their masks (segmented into three classes: sky, small rocks, bigger rocks). Additionally, a csv file of bounding boxes and cleaned masks of ground truths are provided.\n",
        "\n",
        "An interesting feature of this dataset is that the images are synthetic; they were created using Planetside Software's Terragen. This isn't too obvious immediately as the renderings are highly realistic but it does make more sense after taking into account the scarcity of space imagery data.\n",
        "\n",
        "Acknowledgment: Romain Pessia and Genya Ishigami of the Space Robotics Group, Keio University, Japan. You can find the dataset https://www.kaggle.com/romainpessia/artificial-lunar-rocky-landscape-dataset"
      ],
      "metadata": {
        "id": "jcPmSyZzQXgP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reminder to turn on your GPU accelerator, from right hand side of your kaggle notebook, under Settings."
      ],
      "metadata": {
        "id": "9XMsRC-aTdIi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing libraries\n",
        "\n"
      ],
      "metadata": {
        "id": "vneA5pJxQXgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import keras \n",
        "\n",
        "from tqdm import tqdm\n",
        "import glob\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "bGIcq3_DQXgT",
        "execution": {
          "iopub.status.busy": "2022-04-07T17:52:47.454286Z",
          "iopub.execute_input": "2022-04-07T17:52:47.454968Z",
          "iopub.status.idle": "2022-04-07T17:52:52.943503Z",
          "shell.execute_reply.started": "2022-04-07T17:52:47.454929Z",
          "shell.execute_reply": "2022-04-07T17:52:52.942660Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing segmentation_models module\n",
        "\n",
        "Make sure to turn your internet on from kaggle settings on the right hand side. If you don't see the Internet option in Settings, verify your kaggle profile by updating you phone number. \n",
        "\n",
        "### you'll know more about segmentation_models at the end of this lecture and in the next lecture. In this lecture we'll only use segmentation_models for iou_score. "
      ],
      "metadata": {
        "id": "1GJtQO3zTdIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install segmentation_models\n",
        "\n",
        "import segmentation_models as sm\n",
        "\n",
        "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
        "sm.set_framework('tf.keras')\n",
        "keras.backend.set_image_data_format('channels_last')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-07T17:52:52.946800Z",
          "iopub.execute_input": "2022-04-07T17:52:52.947019Z",
          "iopub.status.idle": "2022-04-07T17:53:02.273555Z",
          "shell.execute_reply.started": "2022-04-07T17:52:52.946991Z",
          "shell.execute_reply": "2022-04-07T17:53:02.272628Z"
        },
        "trusted": true,
        "id": "0R7PLw5gTdIl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a21c7f61-0386-418c-b2c2-97f4de399e1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting segmentation_models\n",
            "  Downloading segmentation_models-1.0.1-py3-none-any.whl (33 kB)\n",
            "Collecting image-classifiers==1.0.0\n",
            "  Downloading image_classifiers-1.0.0-py3-none-any.whl (19 kB)\n",
            "Collecting efficientnet==1.0.0\n",
            "  Downloading efficientnet-1.0.0-py3-none-any.whl (17 kB)\n",
            "Collecting keras-applications<=1.0.8,>=1.0.7\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 7.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from efficientnet==1.0.0->segmentation_models) (0.18.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation_models) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation_models) (1.21.6)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications<=1.0.8,>=1.0.7->segmentation_models) (1.5.2)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (1.4.1)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (3.2.2)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (2021.11.2)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (7.1.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (1.3.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (2.6.3)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (2.4.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models) (4.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models) (1.15.0)\n",
            "Installing collected packages: keras-applications, image-classifiers, efficientnet, segmentation-models\n",
            "Successfully installed efficientnet-1.0.0 image-classifiers-1.0.0 keras-applications-1.0.8 segmentation-models-1.0.1\n",
            "Segmentation Models: using `keras` framework.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing Pipeline"
      ],
      "metadata": {
        "id": "OdS9NTtwQXgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Here load_data function is called. This will load the dataset paths and \n",
        "split it into X_train, X_test, y_train, y_test '''\n",
        "\n",
        "img_dir = '../input/artificial-lunar-rocky-landscape-dataset/images/render'\n",
        "mask_dir = '../input/artificial-lunar-rocky-landscape-dataset/images/clean'\n",
        "\n",
        "\n",
        "# let's get the list of image paths and mask paths in sorted order from the given directory respectively\n",
        "images = [os.path.join(img_dir, x) for x in sorted(os.listdir(img_dir))]\n",
        "masks = [os.path.join(mask_dir, x) for x in sorted(os.listdir(mask_dir))]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-07T17:53:02.275690Z",
          "iopub.execute_input": "2022-04-07T17:53:02.275992Z",
          "iopub.status.idle": "2022-04-07T17:53:02.892779Z",
          "shell.execute_reply.started": "2022-04-07T17:53:02.275949Z",
          "shell.execute_reply": "2022-04-07T17:53:02.891932Z"
        },
        "trusted": true,
        "id": "SJ1vD73xTdIp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "7f9ef7f4-0362-474e-cb2b-c211e5b82eb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-bf8fd0935400>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# let's get the list of image paths and mask paths in sorted order from the given directory respectively\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/artificial-lunar-rocky-landscape-dataset/images/render'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's get top 5 images and masks from the lists\n",
        "images[:5], masks[:5]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-07T17:53:02.896667Z",
          "iopub.execute_input": "2022-04-07T17:53:02.897313Z",
          "iopub.status.idle": "2022-04-07T17:53:02.905638Z",
          "shell.execute_reply.started": "2022-04-07T17:53:02.897280Z",
          "shell.execute_reply": "2022-04-07T17:53:02.904804Z"
        },
        "trusted": true,
        "id": "dj-4R4sbTdIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For this session, we will just use first 2000 images and masks as our dataset"
      ],
      "metadata": {
        "id": "4PjqMlM-TdIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images = images[:2000]\n",
        "masks = masks[:2000]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-07T17:53:02.907103Z",
          "iopub.execute_input": "2022-04-07T17:53:02.908038Z",
          "iopub.status.idle": "2022-04-07T17:53:02.915016Z",
          "shell.execute_reply.started": "2022-04-07T17:53:02.907997Z",
          "shell.execute_reply": "2022-04-07T17:53:02.914189Z"
        },
        "trusted": true,
        "id": "oZrnkN5XTdIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_img = Image.open(images[0])\n",
        "sample_img"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-07T17:53:02.916671Z",
          "iopub.execute_input": "2022-04-07T17:53:02.917056Z",
          "iopub.status.idle": "2022-04-07T17:53:03.084809Z",
          "shell.execute_reply.started": "2022-04-07T17:53:02.917017Z",
          "shell.execute_reply": "2022-04-07T17:53:03.084164Z"
        },
        "trusted": true,
        "id": "6EXl8jnlTdIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_mask = Image.open(masks[0])\n",
        "sample_mask"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-07T17:53:03.085678Z",
          "iopub.execute_input": "2022-04-07T17:53:03.086044Z",
          "iopub.status.idle": "2022-04-07T17:53:03.114842Z",
          "shell.execute_reply.started": "2022-04-07T17:53:03.086010Z",
          "shell.execute_reply": "2022-04-07T17:53:03.113953Z"
        },
        "trusted": true,
        "id": "DaZRbOJFTdIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_img.size"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-07T17:53:03.116205Z",
          "iopub.execute_input": "2022-04-07T17:53:03.116573Z",
          "iopub.status.idle": "2022-04-07T17:53:03.123085Z",
          "shell.execute_reply.started": "2022-04-07T17:53:03.116535Z",
          "shell.execute_reply": "2022-04-07T17:53:03.122282Z"
        },
        "trusted": true,
        "id": "6FyHq7sYTdIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Originally, our images size is (720, 480) but we will reduce the size for better and faster processing. Since we are focusing on the clean masks so it will not effect much. \n",
        "\n",
        "### Ground masks are more detailed and have so much noise. We'll keep things easy for our lecture. However, feel free to use ground masks and play around to explore more. "
      ],
      "metadata": {
        "id": "akC9kBTmTdIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we will use this shape of data for our model\n",
        "H = 256 # height of image\n",
        "W = 256 # width of image\n",
        "\n",
        "\n",
        "# for images and labels, to store our dataset\n",
        "X_img = []\n",
        "y_mask = []\n",
        "\n",
        "# here we have our loop to read, process and store our images X_img, and y_mask variables\n",
        "for x, y in tqdm(zip(images, masks)):\n",
        "    # reading images\n",
        "    img = cv2.imread(x, cv2.IMREAD_COLOR)\n",
        "    img = cv2.resize(img, (W, H))\n",
        "    img = img / 255.0\n",
        "    img = img.astype(np.float32) # if pixel values are less than 1, then it is important that the values have float datatype\n",
        "    \n",
        "    # reading masks\n",
        "    mask = cv2.imread(y, cv2.IMREAD_GRAYSCALE)\n",
        "    mask = cv2.resize(mask, (W, H))\n",
        "    mask = mask.astype(np.int32)  # if pixel values are between 1 and 255, then it is important that the values have integer datatype\n",
        "    \n",
        "    # storing images and masks\n",
        "    X_img.append(img)\n",
        "    y_mask.append(mask)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-07T17:53:03.124687Z",
          "iopub.execute_input": "2022-04-07T17:53:03.124920Z",
          "iopub.status.idle": "2022-04-07T17:53:55.318663Z",
          "shell.execute_reply.started": "2022-04-07T17:53:03.124890Z",
          "shell.execute_reply": "2022-04-07T17:53:55.317802Z"
        },
        "trusted": true,
        "id": "-r412gybTdIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_img = np.array(X_img)\n",
        "y_mask = np.array(y_mask)\n",
        "\n",
        "# 1600 datapoints as training dataset and 400 for validation dataset \n",
        "X_train = X_img[:1600]\n",
        "X_valid = X_img[1600:]\n",
        "\n",
        "y_train = y_mask[:1600]\n",
        "y_valid = y_mask[1600:]\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-07T17:53:55.322279Z",
          "iopub.execute_input": "2022-04-07T17:53:55.322970Z",
          "iopub.status.idle": "2022-04-07T17:53:55.965449Z",
          "shell.execute_reply.started": "2022-04-07T17:53:55.322920Z",
          "shell.execute_reply": "2022-04-07T17:53:55.964667Z"
        },
        "trusted": true,
        "id": "7GFo6HUcTdIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape\n",
        "# number of images, height, width, channels"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-07T18:06:45.759144Z",
          "iopub.execute_input": "2022-04-07T18:06:45.760063Z",
          "iopub.status.idle": "2022-04-07T18:06:45.769277Z",
          "shell.execute_reply.started": "2022-04-07T18:06:45.760016Z",
          "shell.execute_reply": "2022-04-07T18:06:45.768526Z"
        },
        "trusted": true,
        "id": "qD4Nzq55TdIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1,2, figsize = (10, 10))\n",
        "\n",
        "ax1.set_title('Image')\n",
        "ax2.set_title('Mask')\n",
        "\n",
        "ax1.imshow(X_train[1])\n",
        "ax2.imshow(y_train[1], cmap = 'gray')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-07T18:09:07.680507Z",
          "iopub.execute_input": "2022-04-07T18:09:07.680815Z",
          "iopub.status.idle": "2022-04-07T18:09:07.997856Z",
          "shell.execute_reply.started": "2022-04-07T18:09:07.680783Z",
          "shell.execute_reply": "2022-04-07T18:09:07.997190Z"
        },
        "trusted": true,
        "id": "8wxdt07iTdIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check this article to know more about how to build optimized data pipeline using tf\n",
        "https://www.tensorflow.org/guide/data_performance"
      ],
      "metadata": {
        "id": "5WP9p5NATdIv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Pipeline\n",
        "\n",
        "### One hot encoding\n",
        "\n",
        "![](https://i.imgur.com/mtimFxh.png)\n",
        "\n",
        "#### Similarly, we'll one hot encode our labels to 4 different channels for four classes"
      ],
      "metadata": {
        "id": "ANA5KGoRTdIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4\n",
        "num_classes = 4 \n",
        "\n",
        "'''Here the from_tensor_slices function is called to make dataset objects of our training and validation sets'''\n",
        "# calling tf_dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, tf.one_hot(y_train, num_classes, dtype=tf.int32)))\n",
        "\n",
        "valid_dataset = tf.data.Dataset.from_tensor_slices((X_valid, tf.one_hot(y_valid, num_classes, dtype=tf.int32)))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-07T17:54:30.359981Z",
          "iopub.execute_input": "2022-04-07T17:54:30.360649Z",
          "iopub.status.idle": "2022-04-07T17:54:34.517337Z",
          "shell.execute_reply.started": "2022-04-07T17:54:30.360605Z",
          "shell.execute_reply": "2022-04-07T17:54:34.516530Z"
        },
        "trusted": true,
        "id": "GEFDAxgITdIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read more about prefetching and AUTOTUNE here: https://www.tensorflow.org/guide/data_performance#optimize_performance\n",
        "\n",
        "## Naive Approach\n",
        "![](https://www.tensorflow.org/guide/images/data_performance/naive.svg)\n",
        "\n",
        "\n",
        "## After prefetching\n",
        "\n",
        "![](https://www.tensorflow.org/guide/images/data_performance/prefetched.svg)"
      ],
      "metadata": {
        "id": "MA6xzMK3TdIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "valid_dataset = valid_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-07T17:54:55.588305Z",
          "iopub.execute_input": "2022-04-07T17:54:55.588875Z",
          "iopub.status.idle": "2022-04-07T17:54:55.600480Z",
          "shell.execute_reply.started": "2022-04-07T17:54:55.588827Z",
          "shell.execute_reply": "2022-04-07T17:54:55.599455Z"
        },
        "trusted": true,
        "id": "KiLkS-hZTdIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = iter(valid_dataset)\n",
        "data = next(sample)\n",
        "data[0].shape\n",
        "# batch size, height, width, channels"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-07T17:55:02.119584Z",
          "iopub.execute_input": "2022-04-07T17:55:02.120401Z",
          "iopub.status.idle": "2022-04-07T17:55:02.711629Z",
          "shell.execute_reply.started": "2022-04-07T17:55:02.120348Z",
          "shell.execute_reply": "2022-04-07T17:55:02.710879Z"
        },
        "trusted": true,
        "id": "lWIoKssMTdIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[1].shape\n",
        "# batch size, height, width, channels/classes"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-07T17:55:04.455678Z",
          "iopub.execute_input": "2022-04-07T17:55:04.456523Z",
          "iopub.status.idle": "2022-04-07T17:55:04.464289Z",
          "shell.execute_reply.started": "2022-04-07T17:55:04.456480Z",
          "shell.execute_reply": "2022-04-07T17:55:04.463432Z"
        },
        "trusted": true,
        "id": "SNNLqwmITdIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating U-net Architecture"
      ],
      "metadata": {
        "id": "1MqxtDTmQXga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, MaxPool2D, UpSampling2D, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "'''conv_block it is used to create one block with two convolution layer \n",
        "followed by BatchNormalization and activation function relu. \n",
        "If the pooling is required then Maxpool2D is applied and return it else not.'''\n",
        "# function to create convolution block\n",
        "def conv_block(inputs, filters, pool=True):\n",
        "    x = Conv2D(filters, 3, padding=\"same\")(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    x = Conv2D(filters, 3, padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    if pool == True:\n",
        "        p = MaxPool2D((2, 2))(x)\n",
        "        return x, p\n",
        "    else:\n",
        "        return x\n",
        "\n",
        "'''build_unet it is used to create the U-net architecture.'''\n",
        "# function to build U-net\n",
        "def build_unet(shape, num_classes):\n",
        "    inputs = Input(shape)\n",
        "\n",
        "    \"\"\" Encoder \"\"\"\n",
        "    x1, p1 = conv_block(inputs, 16, pool=True)\n",
        "    x2, p2 = conv_block(p1, 32, pool=True)\n",
        "    x3, p3 = conv_block(p2, 48, pool=True)\n",
        "    x4, p4 = conv_block(p3, 64, pool=True)\n",
        "\n",
        "    \"\"\" Bridge \"\"\"\n",
        "    b1 = conv_block(p4, 128, pool=False)\n",
        "\n",
        "    \"\"\" Decoder \"\"\"\n",
        "    # Reference for UpSampling2D: https://www.tensorflow.org/api_docs/python/tf/keras/layers/UpSampling2D\n",
        "    # it simply repeats the rows and columns of the data by size[0] and size[1] respectively in nearest interpolation\n",
        "    # check below in the below cell, the difference between bilinear and nearest interpolation\n",
        "    u1 = UpSampling2D(size = (2, 2), interpolation=\"bilinear\")(b1)\n",
        "    c1 = Concatenate()([u1, x4])\n",
        "    x5 = conv_block(c1, 64, pool=False)\n",
        "\n",
        "    u2 = UpSampling2D((2, 2), interpolation=\"bilinear\")(x5)\n",
        "    c2 = Concatenate()([u2, x3])\n",
        "    x6 = conv_block(c2, 48, pool=False)\n",
        "\n",
        "    u3 = UpSampling2D((2, 2), interpolation=\"bilinear\")(x6)\n",
        "    c3 = Concatenate()([u3, x2])\n",
        "    x7 = conv_block(c3, 32, pool=False)\n",
        "\n",
        "    u4 = UpSampling2D((2, 2), interpolation=\"bilinear\")(x7)\n",
        "    c4 = Concatenate()([u4, x1])\n",
        "    x8 = conv_block(c4, 16, pool=False)\n",
        "\n",
        "    \"\"\" Output layer \"\"\"\n",
        "    output = Conv2D(num_classes, 1, padding=\"same\", activation=\"softmax\")(x8)\n",
        "\n",
        "    return Model(inputs, output)"
      ],
      "metadata": {
        "id": "tYCyf8smQXga",
        "execution": {
          "iopub.status.busy": "2022-04-07T17:55:20.527420Z",
          "iopub.execute_input": "2022-04-07T17:55:20.527934Z",
          "iopub.status.idle": "2022-04-07T17:55:20.543573Z",
          "shell.execute_reply.started": "2022-04-07T17:55:20.527894Z",
          "shell.execute_reply": "2022-04-07T17:55:20.542750Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (1, 3, 2, 1)\n",
        "x = np.arange(np.prod(input_shape)).reshape(input_shape)\n",
        "print(x)\n",
        "\n",
        "b = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation = \"bilinear\")(x)\n",
        "print(b)\n",
        "\n",
        "n = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation = \"nearest\")(x)\n",
        "print(n)\n"
      ],
      "metadata": {
        "id": "4D-NgErpUl_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For Contracting Path:** the **conv_block** function is called four time which will create four block with pooling (pool = True). The process is repeated 3 more times.\n",
        "\n",
        "**For Bridge:** the **conv_block** function is called one time without pooling (pool=False).\n",
        "\n",
        "**For Expansive Path: UpSampling2D** is used to expands the size of images. This expanded  image is concatenated with the corresponding image from the contracting path, The reason here is to combine the information from the previous layers in order to get a more precise prediction. And now **conv_block** function is called without pooling (pool=False). The process is repeated 3 more times.\n",
        "\n",
        "The last step is to reshape the image to satisfy our prediction requirements. The last layer is a convolution layer with 1 filter of size 1x1."
      ],
      "metadata": {
        "id": "8wNXCjt4QXgb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calling build_unet function\n",
        "model = build_unet((256, 256, 3), 4)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "65hPnreJQXgb",
        "execution": {
          "iopub.status.busy": "2022-04-07T17:55:26.968596Z",
          "iopub.execute_input": "2022-04-07T17:55:26.969404Z",
          "iopub.status.idle": "2022-04-07T17:55:27.382741Z",
          "shell.execute_reply.started": "2022-04-07T17:55:26.969354Z",
          "shell.execute_reply": "2022-04-07T17:55:27.381997Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load model and compile"
      ],
      "metadata": {
        "id": "bMgeqmX2QXgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing libraries\n",
        "from segmentation_models.metrics import iou_score\n",
        "\n",
        "\"\"\" Hyperparameters \"\"\"\n",
        "lr = 1e-4\n",
        "epochs = 5\n",
        "\n",
        "\"\"\"Model\"\"\"\n",
        "model.compile(loss=\"categorical_crossentropy\", \n",
        "              optimizer=tf.keras.optimizers.Adam(lr), \n",
        "              metrics=[iou_score])\n",
        "\n",
        "\n",
        "train_steps = len(X_train)//batch_size\n",
        "valid_steps = len(X_valid)//batch_size"
      ],
      "metadata": {
        "id": "z91qV2ZwQXgc",
        "execution": {
          "iopub.status.busy": "2022-04-07T18:21:15.434391Z",
          "iopub.execute_input": "2022-04-07T18:21:15.435045Z",
          "iopub.status.idle": "2022-04-07T18:21:15.459216Z",
          "shell.execute_reply.started": "2022-04-07T18:21:15.435002Z",
          "shell.execute_reply": "2022-04-07T18:21:15.458374Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train model"
      ],
      "metadata": {
        "id": "SBhRPBKPQXgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''model.fit is used to train the model'''\n",
        "model_history = model.fit(train_dataset,\n",
        "        steps_per_epoch=train_steps,\n",
        "        validation_data=valid_dataset,\n",
        "        validation_steps=valid_steps,\n",
        "        epochs=epochs\n",
        "    )"
      ],
      "metadata": {
        "id": "4lJgBNVwQXgd",
        "execution": {
          "iopub.status.busy": "2022-04-07T18:21:19.573807Z",
          "iopub.execute_input": "2022-04-07T18:21:19.574455Z",
          "iopub.status.idle": "2022-04-07T18:23:05.181948Z",
          "shell.execute_reply.started": "2022-04-07T18:21:19.574414Z",
          "shell.execute_reply": "2022-04-07T18:23:05.181170Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict from model"
      ],
      "metadata": {
        "id": "9d0U7XJZQXgd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to predict result \n",
        "def predict_image(img_path, mask_path, model):\n",
        "    H = 256\n",
        "    W = 256\n",
        "    num_classes = 4\n",
        "\n",
        "    img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
        "    img = cv2.resize(img, (W, H))\n",
        "    img = img / 255.0\n",
        "    img = img.astype(np.float32)\n",
        "\n",
        "    ## Read mask\n",
        "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "    mask = cv2.resize(mask, (W, H))   ## (256, 256)\n",
        "    mask = np.expand_dims(mask, axis=-1) ## (256, 256, 1)\n",
        "    mask = mask * (255/num_classes)\n",
        "    mask = mask.astype(np.int32)\n",
        "    mask = np.concatenate([mask, mask, mask], axis=2)\n",
        "\n",
        "    ## Prediction\n",
        "    pred_mask = model.predict(np.expand_dims(img, axis=0))[0]\n",
        "    pred_mask = np.argmax(pred_mask, axis=-1)\n",
        "    pred_mask = np.expand_dims(pred_mask, axis=-1)\n",
        "    pred_mask = pred_mask * (255/num_classes)\n",
        "    pred_mask = pred_mask.astype(np.int32)\n",
        "    pred_mask = np.concatenate([pred_mask, pred_mask, pred_mask], axis=2)\n",
        "\n",
        "    return img, mask, pred_mask"
      ],
      "metadata": {
        "id": "ey2fgIyiQXge",
        "execution": {
          "iopub.status.busy": "2022-04-07T18:23:14.057078Z",
          "iopub.execute_input": "2022-04-07T18:23:14.057488Z",
          "iopub.status.idle": "2022-04-07T18:23:14.069074Z",
          "shell.execute_reply.started": "2022-04-07T18:23:14.057448Z",
          "shell.execute_reply": "2022-04-07T18:23:14.068246Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to display result\n",
        "def display(display_list):\n",
        "  plt.figure(figsize=(12, 10))\n",
        "\n",
        "  title = ['Input Image', 'True Mask', 'Predicted Mask', 'Mask On Image']\n",
        "\n",
        "  for i in range(len(display_list)):\n",
        "    plt.subplot(1, len(display_list), i+1)\n",
        "    plt.title(title[i])\n",
        "    plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n",
        "    plt.axis('off')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "y_f35xF5QXge",
        "execution": {
          "iopub.status.busy": "2022-04-07T18:23:16.279699Z",
          "iopub.execute_input": "2022-04-07T18:23:16.279974Z",
          "iopub.status.idle": "2022-04-07T18:23:16.285846Z",
          "shell.execute_reply.started": "2022-04-07T18:23:16.279943Z",
          "shell.execute_reply": "2022-04-07T18:23:16.285122Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_path = '../input/artificial-lunar-rocky-landscape-dataset/images/render/render0041.png'\n",
        "mask_path = '../input/artificial-lunar-rocky-landscape-dataset/images/clean/clean0041.png'\n",
        "\n",
        "img, mask, pred_mask = predict_image(img_path, mask_path, model)\n",
        "\n",
        "display([img, mask, pred_mask])"
      ],
      "metadata": {
        "id": "rhP2yyG-QXge",
        "execution": {
          "iopub.status.busy": "2022-04-07T18:23:19.667627Z",
          "iopub.execute_input": "2022-04-07T18:23:19.668238Z",
          "iopub.status.idle": "2022-04-07T18:23:20.426552Z",
          "shell.execute_reply.started": "2022-04-07T18:23:19.668198Z",
          "shell.execute_reply": "2022-04-07T18:23:20.425793Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## segmentation_model\n",
        "\n",
        "segmentation_models is a python library with Neural Networks for Image Segmentation based on Keras and TensorFlow.\n",
        "\n",
        "The main features of this library are:\n",
        "\n",
        "* High level API (just two lines of code to create model for segmentation)\n",
        "* 4 models architectures for binary and multi-class image segmentation (including legendary Unet)\n",
        "* 25 available backbones for each architecture\n",
        "* All backbones have pre-trained weights for faster and better convergence\n",
        "* Helpful segmentation losses (Jaccard, Dice, Focal) and metrics (IoU, F-score)\n",
        "\n",
        "Learn more: https://segmentation-models.readthedocs.io/en/latest/tutorial.html"
      ],
      "metadata": {
        "id": "I6qn3N2xTdI2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A practical note: different backbones in modern U-Nets\n",
        "\n",
        "So far, you have looked at how the U-Net architecture was implemented in the original work by Ronneberger et al. Over the years, many people have experienced with different setups for U-Nets, including pretraining on e.g. ImageNet and then finetuning to their specific image segmentation tasks.\n",
        "\n",
        "This means that today, you will likely use a U-Net that no longer utilizes the original architecture as proposed above - but it's still a good starting point, because the contractive path, expansive path and the skip connections remain the same.\n",
        "\n",
        "**Common backbones for U-Net architectures these days are ResNet, ResNeXt, EfficientNet and DenseNet architectures. Often, these have been pretrained on the ImageNet dataset, so that many common features have already been learned. By using these backbone U-Nets, initialized with pretrained weights, it's likely that you can reach convergence on your segmentation problem much faster.**\n",
        "\n",
        "That's it! You have now a high-level understanding of U-Net and its components sunglasses.\n",
        "\n",
        "## In the next session, we will learn how you can use segmentation_models using Transfer learning to use UNet architecture with different pretrained models as backbone.\n",
        "\n",
        "## In next week, we will learn about different techniques to improve the accuracy of our model. "
      ],
      "metadata": {
        "id": "9On3t8XsTdI2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Time to practice some questions!"
      ],
      "metadata": {
        "id": "2ABdrOYJLrQP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-1 How will the model result and training be affected if don't reduce the size of the each image taken."
      ],
      "metadata": {
        "id": "vGQIew1eLwoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your answer here\n"
      ],
      "metadata": {
        "id": "-2inUdA6MCsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-2 What is one-hot encoding. Why we use it in while preprocessing our images and is there any alternative to one-hot encoding. "
      ],
      "metadata": {
        "id": "XbLqQTMrMDNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your answer here\n"
      ],
      "metadata": {
        "id": "KHjdYWZlMyBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-3 What will be the effect in the results of the UNet model if we change parameters like Contracting Path(like change the number of times called) and Bridge(like use pooling) in conv_block."
      ],
      "metadata": {
        "id": "0Da_RlYaMxQr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mDq7SGnvNnVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Thank you for completing the notebook!"
      ],
      "metadata": {
        "id": "vxtY3VaUNn2J"
      }
    }
  ]
}